# SVM[1] - 线性可分SVM的目标函数
## 写在前面：线性可分SVM的目标函数
$$\min{\frac12{||w||}^2}, s.t. y^i(w^Tx^i+b)>=1$$
本节希望解决的几个问题：
1. 目标函数的由来
2. 约束条件的由来（为什么是≥1）
## 重新审视逻辑回归
逻辑回归的判别方法是：对于某个待判别的样本$x$，求$h_{\theta}(x)$，如果大于0.5就判决为正样本，反之判决为负样本。  
进一步地，实际上判别的决定权仅仅在于${\theta}^Tx$：当${\theta}^Tx>>0$时，判决为正样本，反之判决为负样本。  
逻辑回归就是要学习得到$\theta$，使得正例的${\theta}^Tx$远大于0，负例的${\theta}^Tx$远小于0，强调在全部训练集上达到该目标。
## 从逻辑回归到最大间隔分类器
${\theta}^Tx=0$， 或$w^Tx+b=0$即分类超平面。  
希望${\theta}^Tx$远大于0（正样本）或远小于0（负样本），从几何直观上解释如下：  
越远离超平面的点越容易分隔，且泛化能力较好；  
反之，越接近超平面的点越“难”分隔，且泛化能力较差。  
基于上述思路，我们希望得到的分类器（分类超平面）距离全部数据点的最小距离越大越好，即**最大间隔分类器**
## 最大间隔分类器的目标函数
$$argmax_{w, b}{\min_{i}{\frac{y^i(w^Tx^i+b)}{||w||}}}$$
## 化简目标函数
基本原则是：$w$和$b$等比例缩放，超平面$w^Tx+b=0$保持不变。  
因此，总能找到一种合适的缩放，使得$y^i(w^Tx^i+b)>=1$
目标函数转化为：
$$\max{\frac{1}{||w||}}, s.t. y^i(w^Tx^i+b)>=1$$
进一步转化为：
$$\min{\frac12{||w||}^2}, s.t. y^i(w^Tx^i+b)>=1$$