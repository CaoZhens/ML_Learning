# 机器学习的数学基础[9] - 熵的本质
## 信息
**信息**是个很抽象的概念。人们常说“信息很多，或者信息很少”，但却很难说清楚（量化）信息到底有多少。  
比如，[本repo]()的内容到底有多少信息量？  
1948年，香农提出了信息量和信息熵的概念，才解决了信息的量化度量问题。
## 信息的度量——香农信息量
信息量是对信息的度量，正如时间的度量是秒；长度的度量是米。  
信息量具有以下两个特性：
1. 信息量的大小与信息的不确定性存在着直接的关系，即：信息量随着事件的发生概率增大而递减，且不小于零。
2. 两个不相关事件同时发生时的信息量，应该等于两个事件各自发生时的信息量之和。
根据以上两个特性，香农信息量定义如下：
$$h(x) = \log{\frac{1}{p(x)}} = -\log{p(x)}$$
## 熵的本质
### 熵的本质一：香农信息量的期望
熵是香农信息量$\log{\frac{1}{p(x)}}$的期望，即：
$$H(X) = -\sum_x p(x) \log{p(x)}$$
### 熵的本质二：事件的不确定性
信息熵代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性越大。
### 熵的本质三：编码方案完美时，最短平均编码长度
信息熵衡量了系统的不确定性，换一种说法：  
当我们要消除系统的不确定性时，所要付出的**最小努力**的大小就是信息熵。