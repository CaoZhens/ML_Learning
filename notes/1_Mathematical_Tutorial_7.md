# 机器学习的数学基础[9] - 熵的本质
## 香农信息量
信息量是对信息的度量，正如时间的度量是秒；长度的度量是米。  
信息量具有以下两个特性：
1. 信息量随着事件的发生概率增大而递减，且不小于零。
2. 两个不相关事件，同时发生时的信息量应该等于两个事件各自发生时的信息量之和。
根据以上两个特性，香农信息量定义如下：
$$h(x) = \log_2{\frac{1}{p(x)}} = -\log_2{p(x)}$$

## 熵的本质一：香农信息量的期望
$$H(X) = -\sum_x p(x) \log{p(x)}$$