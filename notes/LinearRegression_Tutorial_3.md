# 线性回归（3）最大似然估计与线性回归的先验假设

## 前述求解思路的局限

前述求解思路是用一个特定模型（线性模型），尽量匹配数据并使误差函数尽可能小

前述求解中包含着两处**直接定义**

首先定义了一个线性模型
$$h_\theta(\mathbf{x}) = \theta_{0}x_{0}+\theta_{1}x_{1}+\cdots+\theta_{n}x_{n}
$$

其次定义了一个误差函数
$$
J_m(\theta) = \frac{1}{2}\sum_{i=1}^{m}(\hat{y}^i - y^i)^2 = \frac{1}{2}\sum_{i=1}^{m}(\theta^T \mathbf{x}^i - y^i)^2
$$

**提问：模型和误差函数为什么定义成上述形式？**

直接回答这两个问题是很困难的，让我们换个角度，从概率论的角度来讨论

## 从概率角度看问题：极大似然估计

已知一组样本，让我们定义某个系统，系统参数为$\Theta$
当$P(\mathbf{y}|\mathbf{X};\Theta)$取最大值时，对应的系统为最优匹配系统

**即：极大似然估计**

**通过上述定义，我们将求解线性回归模型问题转化为极大似然估计问题**

## 线性回归的两个先验假设

**首先，假设各样本独立同分布**

同分布是进行极大似然估计的前提条件；独立性会简化我们的推导过程

**其次，给出线性回归模型最重要的先验假设，假设模型预测值与实际值之间满足正态分布，即**

$$
y^i = h_\theta(x^i) + \varepsilon^i, \varepsilon  \sim N(0, \sigma^2) 
$$

理论基础：中心极限定理

直观解释：如果一个事物受到多种相互独立因素的影响，不管每个因素本身是什么分布，它们加总后，结果的平均值就是正态分布






