#机器学习的数学基础[10] - 机器学习中的交叉熵损失
## 机器学习的目的
现有关于样本数据集的两个概率分布，记为$p$和$q$。其中$p$为真实分布，$q$为非真实分布。  
在机器学习领域，$p$可表示为真实样本的分布，$q$则为训练后的模型预测的分布。  
**机器学习的目的就是希望$q$尽可能地逼近，甚至等于$p$。**
## 衡量两个概率分布的差异性——相对熵
根据[机器学习的数学基础[8] - 信息论基础2 - 相对熵／KL散度／交叉熵](./1_Mathematical_Tutorial_6.md):  
> 相对熵用于衡量两个概率分布之间的差异性，即：
$$\mathbf{D}(p||q) = \sum_x{P(x)\log{\frac{P(x)}{Q(x)}}} = -\sum_x{P(x)\log{Q(x)}} - \left( -\sum_x{P(x)\log{P(x)}} \right)$$
其又被称为KL散度。

即：相对熵 = 某个模型下的交叉熵 - 信息熵（根据样本真实分布计算而得的信息熵）。  
机器学习的目的就是希望相对熵尽可能小。
## 机器学习的优化目标——交叉熵最小
在具体的机器学习问题中，样本的真实概率分布总是固定的，因此，相对熵公式的后半部分就成了一个常数。  
那么相对熵达到最小值的时候，也意味着交叉熵达到了最小值。  
对$q$的优化就等效于求交叉熵的最小值